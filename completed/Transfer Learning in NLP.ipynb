{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Jupyter Candies \n\n# Run the notebook readable w/o wanrings.\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install the additional libaries.\n! pip install transformers\n! pip install torch==1.2.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe first section of this notebook will first ***very*** briefly introduce some background concepts that's good to know about \n\n- The ImageNet Moment in NLP\n- A Zoo of Pre-trained Models\n- BERT (Bidirectional Encoder Representation from Transformers) Basics, one of the more popular transfer learning models for NLP and \n\n\nThe second section** demonstrates how you can the BERT model from `pytorch_transformer` library to: \n\n1. **Convert text to array/list of floats** \n2. **Fill in the blanks** \n\n<!--\n3. **Fine-tune the pre-trained model** based on the data you want to use for a specific task\n4. **Apply the fine-tuned model** to a couple of downstream tasks\n-->\n\n### References\n\nI'll strongly recommend these readings to better understand/appreciate the first part of the notebook =)\n\n - [Rush (2018) blogpost](https://nlp.seas.harvard.edu/2018/04/03/attention.html) on \"The Annotated Transformer\" that explains the explaining the Transformer architecture \n - [Ruder et al. (2019) tutorial](http://ruder.io/state-of-transfer-learning-in-nlp/index.html) on \"Transfer Learning in NLP\" @ NAACL\n - [Weng (2019) blogpost](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html) on \"Generalized Language Models\"\n - https://github.com/huggingface/transformers\n - https://github.com/explosion/spacy-transformers\n "},{"metadata":{},"cell_type":"markdown","source":"## Background: The ImageNet Moment for NLP \n\n\nTransfer learning gained traction in Computer Vision, made popular by the [ImageNet](http://www.image-net.org) and [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) image classification task. Similarly, transfer learning gained popularity when a wave of Transformer based models, with the BERT model being the more popular one from the zoo.\n"},{"metadata":{},"cell_type":"markdown","source":"## Background: A Zoo of Pre-trained Models\n\nThere's a whole variety of transfer learning pre-trained models in the wild. [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) puts them nicely into a chart of the no. of parameters* of the model with respect to the dates the models were released: \n\n<img src=\"https://miro.medium.com/max/4140/1*IFVX74cEe8U5D1GveL1uZA.png\" alt=\"DistilBERT\" style=\"width:700px;\"/>\n\n***Note:** \"*Parameters*\" approximates to how much \"*memory*\"/\"*information*\" the model is storing after pre-training.\n\n\n<!-- \nHere's a summary inspired by [Weng's (2019) blogpost](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html):\n\n| Name | Architecture | Autoregressive | No. of Parameters | Release Date | Pre-training | Downstream tasks | Downstream Model | \n|:-|:-|:-:|:-:|:-:|:-:|:-:|:-|\n| [ELMo](https://allennlp.org/elmo) | 2-layers BiLSTM | Yes | 94M | Apr 2018 | Unsupervised | Feature-based | Task-agnostic | None | \n| [ULMFit](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) | AWD-LSTM | Yes | ?? | Apr 2018 | Unsupervised | Feature-based | Task-agnostic | None | \n| [GPT](https://openai.com/blog/language-unsupervised/) | Transformer Decoder | Yes | 110M | Jul 2018 | Unsupervised | Model-baed | Task-agnostic | Pre-trained layers + Task layers | \n| [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) | Transformer Encoder | No | 340M | Oct 2018 | Unsupervised | Model-based | Task-agnostic | Pre-trained layers + Task layers | \n| [Transfomer ElMo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/training_transformer_elmo.md) | Transformer Decoder | Yes | 465M | Jan 2019 | Unsupervised | Task-agnostic | Pre-trained layers + Task layers | \n| [GPT-2](https://openai.com/blog/better-language-models/) | Transformer Decoder | Yes | 1500M | Feb 2019 | Unsupervised | Model-baed | Task-agnostic | Pre-trained layers + Task layers | \n\n-->"},{"metadata":{},"cell_type":"markdown","source":"### Background: BERT Basics\n\nLets start with the elephant in the zoo!\n\n<img src=\"https://lilianweng.github.io/lil-log/assets/images/BERT-input-embedding.png\" alt=\"BERTInputs\" style=\"width:700px;\"/>\n\nFirst the input string needs to prepended with the `[CLS]` token, this special token is used to allocate some placeholder than can be used to produce the labels for classification task. \n\nThen, for each sentence that's inside the text string, explicit `[SEP]` tokens need to be added to indicate on of a sentence. \n\n\nThen string input needs to be converted to three components before passing them to Transformer model:\n\n - **WordPiece tokenization**: The text (string) input would be split into tokens segmented using the WordPiece model that may split natural words further into sub-words units to handle rare/unknown words. \n \n - **Segment Indices**: This part indicates the start and end of the sentences in the string inputs, delimited by the special `[SEP]` token.\n \n - **Position Indices**: This part simply enumerates the index of WordPiece tokens. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from itertools import chain\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel, BertForMaskedLM\n\n# Load pre-trained model tokenizer (vocabulary)\n# A tokenizer will split the text into the appropriate sub-parts (aka. tokens).\n# Depending on how the pre-trained model is trained, the tokenizers defers.\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Example of a tokenized input after WordPiece Tokenization.\ntext = \"[CLS] my dog is cute [SEP] he likes playing [SEP]\"\ntokenizer.tokenize(text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gotcha! The output is differen from texample in the image above!!\n\nThat's because the full word `playing` is inside the `BertTokenizer`'s WordPiece vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"playing\" in tokenizer.wordpiece_tokenizer.vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets try another verb that's not in the vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"slacking\" in tokenizer.wordpiece_tokenizer.vocab)\n\ntext = \"[CLS] my dog is cute [SEP] he likes slacking [SEP]\"\ntokenized_text = tokenizer.tokenize(text)  # There, we see the ##ing token!\ntokenized_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We fetch the index of these words from the model's vocabulary. "},{"metadata":{"trusted":true},"cell_type":"code","source":"token_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\ntoken_indices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Corresponding to the text input, we need to create the \"segment indices\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to create an array that indicates the end of sentences, delimited by [SEP]\ntext = \"[CLS] my dog is cute [SEP] he likes slacking [SEP]\"\ntokenized_text = tokenizer.tokenize(text)  # There, we see the ##ing token!\n\n# First we find the indices of `[SEP]`, and incrementally adds it up. \n# Here's some Numpy gymnastics... (Thanks to @divakar https://stackoverflow.com/a/58316889/610569)\nm = np.asarray(tokenized_text) == \"[SEP]\"\nsegments_ids = m.cumsum()-m\nsegments_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, we convert the list and numpy arrays to PyTorch's Tensor objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_tensor, segments_tensors = torch.tensor([token_indices]), torch.tensor([segments_ids])\n\n# See the type change?\nprint(tokens_tensor.shape, type(token_indices), type(tokens_tensor))\nprint(segments_tensors.shape, type(segments_ids), type(segments_tensors))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets convert our input text to an array of number!!!"},{"metadata":{},"cell_type":"markdown","source":"### First, we load the pre-trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# When using the BERT model for \"encoding\", i.e. convert string to array of floats, \n# we use the `BertModel` object from pytorch transformer library.\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict hidden states features for each layer\nwith torch.no_grad():\n    encoded_layers, _ = model(tokens_tensor, segments_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the shape is 3-Dimension, i.e. (`batch_size`, `sequence_length`, `hidden_dimension`), where\n\n - `batch_size` corresponds to \"no. of sentences\"\n - `sequence_length` corresponds to \"no. of tokens\"\n - `hidden_dimensions` refers to the \"information for each word provided by the pre-trained model\""},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_layers.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The BERT model is very good at fill-in-the-blank task\n\nThe BERT model is trained using a \"cloze\" task where words are randomly replaced with the `[MASK]` symbols and the model learns to adjust its parameters such that it learns which words are most probable to fit into the `[MASK]` symbols.\n\nWhen using the BERT model for \"guessing missing words\", we use the `BertForMaskedLM` object from pytorch transformer library. Here's an example if we blank out words in the sentence, BERT is able to find the appropriate word to fill it in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to create an array that indicates the end of sentences, delimited by [SEP]\ntext = \"[CLS] please don't let the [MASK] out of the [MASK] . [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\ntoken_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\n\n# Create the segment indices.\nm = np.asarray(tokenized_text) == \"[SEP]\"\nsegments_ids = m.cumsum()-m\n\n# Convert them to the arrays to pytorch tensors.\ntokens_tensor, segments_tensors = torch.tensor([token_indices]), torch.tensor([segments_ids])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the model to the inputs.\nwith torch.no_grad(): # You can take this context manager to mean that we're not training.\n    outputs, *_ = model(tokens_tensor, token_type_ids=segments_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we see that the output tensor shape is different. The dimensions now refers to the (`batch_size`, `sequence_length`, `vocab_size`), where: \n\n - `batch_size` corresponds to \"no. of sentences\"\n - `sequence_length` corresponds to \"no. of tokens\"\n - `vocab_size` is the no. of wordpiece tokens in the tokenizer's vocabulary, we'll use this to fetch the correct word that we want to use to fill in the `[MASK]` symbol."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to check where the masked token is from the original text. \nmask_index = tokenized_text.index('[MASK]') \nprint(mask_index) # The 7th token.\n\n# Then we fetch the vector for the 7th value, \n# The [0, mask_index] refers to accessing vector of vocab_size for\n# the 0th sentence, mask_index-th token.\noutput_value = outputs[0, mask_index]\n\n# As a sanity check we can see that the shape of the output_value\n# is the same as the `vocab_size` from the outputs' shape.\nprint(output_value.shape, \n      output_value.shape[0] == outputs.shape[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets recap the original sentence with the masked word.\nprint(text)\n\n# We have to check where the first masked token is from the original text. \nmask_index = tokenized_text.index('[MASK]') \noutput_value = outputs[0, mask_index]\n## We use torch.argmax to get the index with the highest value.\nmask_word_in_vocab = int(torch.argmax(output_value))\nprint(tokenizer.convert_ids_to_tokens([mask_word_in_vocab]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets recap the original sentence with the masked word.\nprint(text)\n\n# We have to check where the first masked token is from the original text. \n\nfor mask_index, token in enumerate(tokenized_text):\n    if token == '[MASK]':\n        output_value = outputs[0, mask_index]\n        mask_word_in_vocab = int(torch.argmax(output_value))\n        print(tokenizer.convert_ids_to_tokens([mask_word_in_vocab]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets make the fill-in-the-blank feature into a function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_in_the_blanks(text, model, return_str=False):\n    tokenized_text = tokenizer.tokenize(text)\n    token_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\n    # Create the segment indices.\n    m = np.asarray(tokenized_text) == \"[SEP]\"\n    segments_ids = m.cumsum()-m\n    # Convert them to the arrays to pytorch tensors.\n    tokens_tensor = torch.tensor([token_indices])\n    segments_tensors = torch.tensor([segments_ids])\n    \n    # Apply the model to the inputs.\n    with torch.no_grad(): # You can take this context manager to mean that we're not training.\n        outputs, *_ = model(tokens_tensor, token_type_ids=segments_tensors)\n    \n    output_tokens = []\n    for mask_index, token_id in enumerate(token_indices):\n        token = tokenizer.convert_ids_to_tokens([token_id])[0]\n        if token == '[MASK]':\n            output_value = outputs[0, mask_index]\n            # The masked word index in the vocab.\n            mask_word_in_vocab = int(torch.argmax(output_value))\n            token = tokenizer.convert_ids_to_tokens([mask_word_in_vocab])[0]\n        output_tokens.append(token)\n        \n    return \" \".join(output_tokens).replace(\" ##\", \"\") if return_str else output_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval()\n\ntext = \"[CLS] please don't let the [MASK] out of the [MASK] . [SEP] the [MASK] [MASK] [MASK] ran [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model, return_str=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"[CLS] i like to drink beer and eat [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model, return_str=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"[CLS] i like to drink coffee and eat [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model, return_str=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine-tuning BERT models. \n\nBy default, the pre-trained model is trained on the\n\n - BookCorpus, ~800M words\n - English Wikipedia, ~2500M words\n \nIf we want the model to adapt to a specific domain, we need to ***fine-tune*** the model. This section demonstrate how this can be done with the same PyTorch Transformer Library.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"phoenix_turtle = \"\"\"Truth may seem but cannot be;\\nBeauty brag but ’tis not she;\\nTruth and beauty buried be.\"\"\"\nsonnet20 = \"\"\"A woman’s face with Nature’s own hand painted\\nHast thou, the master-mistress of my passion;\\nA woman’s gentle heart, but not acquainte\\nWith shifting change, as is false women’s fashion;\"\"\"\nsonnet1 = \"\"\"From fairest creatures we desire increase,\\nThat thereby beauty’s rose might never die,\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\"\"\"\nsonnet73 = \"\"\"In me thou see’st the glowing of such fire,\\nThat on the ashes of his youth doth lie,\\nAs the death-bed whereon it must expire,\\nConsum’d with that which it was nourish’d by.\"\"\"\nvenus_adonis = \"\"\"It shall be cause of war and dire events,\\nAnd set dissension ‘twixt the son and sire;\\nSubject and servile to all discontents,\\nAs dry combustious matter is to fire:\\nSith in his prime Death doth my love destroy,\\nThey that love best their loves shall not enjoy\\n\"\"\"\nsonnet29 = \"\"\"When, in disgrace with fortune and men’s eyes,\\nI all alone beweep my outcast state,\\nAnd trouble deaf heaven with my bootless cries,\\nAnd look upon myself and curse my fate,\"\"\"\nsonnet130 = \"\"\"I have seen roses damask’d, red and white,\\nBut no such roses see I in her cheeks;\\nAnd in some perfumes is there more delight\\nThan in the breath that from my mistress reeks.\"\"\"\nsonnet116 = \"\"\"Love’s not Time’s fool, though rosy lips and cheeks\\nWithin his bending sickle’s compass come;\\nLove alters not with his brief hours and weeks,\\nBut bears it out even to the edge of doom.\"\"\"\nsonnet18 = \"\"\"But thy eternal summer shall not fade\\nNor lose possession of that fair thou ow’st;\\nNor shall Death brag thou wander’st in his shade,\\nWhen in eternal lines to time thou grow’st;\\nSo long as men can breathe or eyes can see,\\nSo long lives this, and this gives life to thee.\"\"\"\nanthony_cleo = \"\"\"She made great Caesar lay his sword to bed;\\nHe plowed her, and she cropped.\"\"\"\n\nshakespeare = [phoenix_turtle, sonnet20, sonnet1, sonnet73, venus_adonis,\n              sonnet29, sonnet130, sonnet116, sonnet18, anthony_cleo]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertConfig, BertForMaskedLM, BertTokenizer\n\n# Load the BERT model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval()\n# Load the BERT Tokenizer.\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n# Load the BERT Config.\nconfig = BertConfig.from_pretrained('bert-large-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\nimport torch.nn.functional as F\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        \"\"\"\n        :param texts: A list of documents, each document is a list of strings.\n        :rtype texts: list(string)\n        \"\"\"\n        tokenization_process = lambda s: tokenizer.build_inputs_with_special_tokens(\n                                             tokenizer.convert_tokens_to_ids(\n                                                 tokenizer.tokenize(s.lower())))\n        pad_sent = lambda x: np.pad(x, (0,tokenizer.max_len_single_sentence - len(x)), 'constant', \n                                    constant_values=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n        self.examples = torch.tensor([pad_sent(tokenization_process(doc)) for doc in texts])\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item])\n\n# Initialize the Dataset object.\ntrain_dataset = TextDataset(shakespeare, tokenizer)\n# Initalize the DataLoader object, `batch_size=2` means reads 2 poems at a time.\ndataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 poems with 510 tokens per poems, \n# if poem has <510, pad with the 0th index.\ntrain_dataset.examples.shape\n\n# For each batch, we read 2 poems at a time.\nprint(next(iter(dataloader)).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# An example of a batch.\nnext(iter(dataloader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_tokens(inputs, tokenizer, mlm_probability=0.8):\n    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n    probability_matrix = torch.full(labels.shape, mlm_probability)\n    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n\n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW, WarmupLinearSchedule\n\nArguments = namedtuple('Arguments', ['learning_rate', 'weight_decay', 'adam_epsilon', 'warmup_steps', \n                                     'max_steps', 'num_train_epochs'])\n\nargs = Arguments(learning_rate=5e-5, weight_decay=0.0, adam_epsilon=1e-8, warmup_steps=0, # Optimizer arguments\n                 max_steps=10, num_train_epochs=10  # Training routine arugments\n                )  \n\n# Prepare optimizer and schedule (linear warmup and decay)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\nscheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=args.max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _e in range(args.num_train_epochs):\n    print(_e)\n    for step, batch in enumerate(iter(dataloader)):\n        # Randomly mask the tokens 80% of the time. \n        inputs, labels = mask_tokens(batch, tokenizer)\n        # Initialize the model to train mode.\n        model.train()\n        # Feed forward the inputs through the models.\n        loss, _ = model(inputs, masked_lm_labels=labels)\n        # Backpropagate the loss.\n        loss.backward()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}